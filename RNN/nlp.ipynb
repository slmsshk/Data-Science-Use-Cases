{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text = \"\"\"This module provides regular expression matching operations similar to those found in Perl.\n",
    "\n",
    "Both patterns and strings to be searched can be Unicode strings (str) as well as 8-bit strings (bytes). However, Unicode strings and 8-bit strings cannot be mixed: that is, you cannot match a Unicode string with a byte pattern or vice-versa; similarly, when asking for a substitution, the replacement string must be of the same type as both the pattern and the search string.\n",
    "\n",
    "Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write '\\\\\\\\' as the pattern string, because the regular expression must be \\\\, and each backslash must be expressed as \\\\ inside a regular Python string literal. Also, please note that any invalid escape sequences in Python’s usage of the backslash in string literals now generate \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'h\\\\\\\\i'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "req = requests.get('https://www.dr-chuck.com/py4inf/code/mbox-short.txt')\n",
    "print(req.text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = req.text\n",
    "\n",
    "# for i in text.splitlines()[:20]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for line in text.splitlines():\n",
    "    line = line.rstrip()\n",
    "    if re.search('From:', line):\n",
    "        print(line)\n",
    "    # print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in text.splitlines():\n",
    "    line = line.rstrip()\n",
    "    if re.search('^X\\S*: [0-9.]+', line):\n",
    "        print(line)\n",
    "        res = re.findall('[0-9.]+', line)\n",
    "        print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('genesis')\n",
    "nltk.download('book')\n",
    "nltk.download_gui()\n",
    "\n",
    "\n",
    "# from nltk.bbook import *\n",
    "\n",
    "# nltk.download('book')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(book_text)\n",
    "\n",
    "# Remove punctuation\n",
    "tokens = [word for word in tokens if word.isalnum()]\n",
    "\n",
    "# Join the tokens back into a string\n",
    "cleaned_text = ' '.join(tokens)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Example string\n",
    "example_string = \"Hello, world! This is a test string with punctuation.\"\n",
    "\n",
    "# Define the regex pattern for punctuation\n",
    "pattern = r'[^\\w\\s]'\n",
    "\n",
    "# Remove punctuation using re.sub()\n",
    "cleaned_string = re.sub(pattern, '', example_string)\n",
    "\n",
    "print(\"Original string:\", example_string)\n",
    "print(\"String without punctuation:\", cleaned_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Slmss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'runs', 'runner', 'easily', 'fair', 'fairness']\n",
      "Stemmed words: ['run', 'run', 'runner', 'easili', 'fair', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Dummy data\n",
    "text = \"running runs runner easily fair fairness\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Running runs runner easily fair fairness!@#$\n",
      "Normalized text: running runs runner easily fair fairness\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Example text\n",
    "example_text = \"Running runs runner easily fair fairness!@#$\"\n",
    "\n",
    "# Convert text to lowercase\n",
    "normalized_text = example_text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "normalized_text = normalized_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(\"Original text:\", example_text)\n",
    "print(\"Normalized text:\", normalized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the normalized text\n",
    "tokenized_words = word_tokenize(normalized_text)\n",
    "\n",
    "print(\"Tokenized words:\", tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d622f214c2a0420c865c22c6e9d1d662-0\" class=\"displacy\" width=\"590\" height=\"227.0\" direction=\"ltr\" style=\"max-width: none; height: 227.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Running</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">runs</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">runner</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">easily</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">fair</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">fairness!@#$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d622f214c2a0420c865c22c6e9d1d662-0-0\" stroke-width=\"2px\" d=\"M70,92.0 C70,47.0 135.0,47.0 135.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d622f214c2a0420c865c22c6e9d1d662-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">csubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,94.0 L62,82.0 78,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d622f214c2a0420c865c22c6e9d1d662-0-1\" stroke-width=\"2px\" d=\"M160,92.0 C160,47.0 225.0,47.0 225.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d622f214c2a0420c865c22c6e9d1d662-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M225.0,94.0 L233.0,82.0 217.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d622f214c2a0420c865c22c6e9d1d662-0-2\" stroke-width=\"2px\" d=\"M340,92.0 C340,47.0 405.0,47.0 405.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d622f214c2a0420c865c22c6e9d1d662-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,94.0 L332,82.0 348,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d622f214c2a0420c865c22c6e9d1d662-0-3\" stroke-width=\"2px\" d=\"M430,92.0 C430,47.0 495.0,47.0 495.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d622f214c2a0420c865c22c6e9d1d662-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,94.0 L422,82.0 438,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-d622f214c2a0420c865c22c6e9d1d662-0-4\" stroke-width=\"2px\" d=\"M160,92.0 C160,2.0 500.0,2.0 500.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-d622f214c2a0420c865c22c6e9d1d662-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M500.0,94.0 L508.0,82.0 492.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "example_text = \"Running runs runner easily fair fairness!@#$\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Display the POS tags with displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "New York City GPE\n",
      "next year DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a new text\n",
    "new_text = \"Google is planning to open a new office in New York City next year.\"\n",
    "new_doc = nlp(new_text)\n",
    "\n",
    "# Extract named entities from the new text\n",
    "for ent in new_doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and POS tags: [('Microsoft', 'PROPN'), ('Corporation', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('American', 'ADJ'), ('multinational', 'ADJ'), ('technology', 'NOUN'), ('company', 'NOUN'), ('with', 'ADP'), ('headquarters', 'NOUN'), ('in', 'ADP'), ('Redmond', 'PROPN'), (',', 'PUNCT'), ('Washington', 'PROPN'), ('.', 'PUNCT')]\n",
      "Named Entities: [('Microsoft Corporation', 'ORG'), ('American', 'NORP'), ('Redmond', 'GPE'), ('Washington', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "fresh_text = \"Microsoft Corporation is an American multinational technology company with headquarters in Redmond, Washington.\"\n",
    "\n",
    "# Process the text\n",
    "fresh_doc = nlp(fresh_text)\n",
    "\n",
    "# Extract tokens, POS tags, and named entities\n",
    "tokens = [(token.text, token.pos_) for token in fresh_doc]\n",
    "entities = [(ent.text, ent.label_) for ent in fresh_doc.ents]\n",
    "\n",
    "print(\"Tokens and POS tags:\", tokens)\n",
    "print(\"Named Entities:\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Slmss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  quick/JJ\n",
      "  brown/NN\n",
      "  fox/NN\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  the/DT\n",
      "  lazy/JJ\n",
      "  dog/NN)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize and POS tag the text\n",
    "words = word_tokenize(text)\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Perform chunking\n",
    "chunks = ne_chunk(pos_tags)\n",
    "\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# text\n",
    "text = \"The quick brown fox jumps over the lazy dog. Email me at example@example.com.\"\n",
    "\n",
    "# 1. Matching a pattern\n",
    "pattern = r'\\bfox\\b'\n",
    "match = re.search(pattern, text)\n",
    "if match:\n",
    "    print(f\"Found '{match.group()}' at position {match.start()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Finding all matches\n",
    "pattern = r'\\b\\w{3}\\b'  # Words with exactly 3 letters\n",
    "matches = re.findall(pattern, text)\n",
    "print(\"Words with exactly 3 letters:\", matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Replacing text\n",
    "pattern = r'lazy'\n",
    "replacement = 'energetic'\n",
    "new_text = re.sub(pattern, replacement, text)\n",
    "print(\"Replaced text:\", new_text)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Splitting text\n",
    "pattern = r'\\s+'  # Split by whitespace\n",
    "words = re.split(pattern, text)\n",
    "print(\"Split words:\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extracting email addresses\n",
    "pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "emails = re.findall(pattern, text)\n",
    "print(\"Extracted emails:\", emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['and' 'as' 'can' 'chair' 'crutches' 'fast' 'get' 'go' 'hand' 'have' 'her'\n",
      " 'hilly' 'in' 'is' 'keep' 'me' 'mum' 'my' 'off' 'on' 'put' 'shakily'\n",
      " 'shouts' 'sit' 'splints' 'stop' 'straps' 'tear' 'terrain' 'the' 'them'\n",
      " 'then' 'tough' 'up' 'velcro' 'wheel' 'you' 'zoo']\n",
      "Bag of Words array: [[1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1\n",
      "  1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# read a pdf file using python; and then perform these techniques until count vec\n",
    "texts = [\"\"\"The zoo is tough terrain; hilly.\n",
    "I wheel as fast as I can —\n",
    "then Mum shouts ‘Keep up!’\n",
    "I stop. ‘Hand me my crutches.’\n",
    "I shakily get up; tear off my splints’\n",
    "velcro straps, and put them on her.\n",
    "I sit her in the chair. ‘You have a go.’\"\"\"]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Convert the result to an array\n",
    "bow_array = X.toarray()\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Bag of Words array:\", bow_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "# Download the stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Generate a large corpus of words\n",
    "words = [\"machine\", \"learning\", \"deep\", \"neural\", \"networks\", \"artificial\", \"intelligence\", \"data\", \"science\", \"python\", \"programming\", \"language\", \"model\", \"training\", \"algorithm\", \"accuracy\", \"precision\", \"recall\", \"evaluation\", \"metrics\"]\n",
    "\n",
    "# Add stopwords to the corpus\n",
    "corpus_with_stopwords = words + list(stop_words)\n",
    "\n",
    "# Shuffle the corpus to mix stopwords with other words\n",
    "random.shuffle(corpus_with_stopwords)\n",
    "\n",
    "# Join the words to form a large text corpus\n",
    "large_text_corpus = ' '.join(corpus_with_stopwords)\n",
    "\n",
    "print(\"Large text corpus with stopwords:\", large_text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a\\na's\\nable\\nabout\\nabove\\naccording\\naccordingly\\nacross\\nactually\\nafter\\nafterwards\\nagain\\nagainst\\nain't\\nall\\nallow\\nallows\\nalmost\\nalone\\nalong\\nalready\\nalso\\nalthough\\nalways\\nam\\namong\\namongst\\nan\\nand\\nanother\\nany\\nanybody\\nanyhow\\nanyone\\nanything\\nanyway\\nanyways\\nanywhere\\napart\\nappear\\nappreciate\\nappropriate\\nare\\naren't\\naround\\nas\\naside\\nask\\nasking\\nassociated\\nat\\navailable\\naway\\nawfully\\nb\\nbe\\nbecame\\nbecause\\nbecome\\nbecomes\\nbecoming\\nbeen\\nbefore\\nbeforehand\\nbehind\\nbeing\\nbelieve\\nbelow\\nbeside\\nbesides\\nbest\\nbetter\\nbetween\\nbeyond\\nboth\\nbrief\\nbut\\nby\\nc\\nc'mon\\nc's\\ncame\\ncan\\ncan't\\ncannot\\ncant\\ncause\\ncauses\\ncertain\\ncertainly\\nchanges\\nclearly\\nco\\ncom\\ncome\\ncomes\\nconcerning\\nconsequently\\nconsider\\nconsidering\\ncontain\\ncontaining\\ncontains\\ncorresponding\\ncould\\ncouldn't\\ncourse\\ncurrently\\nd\\ndefinitely\\ndescribed\\ndespite\\ndid\\ndidn't\\ndifferent\\ndo\\ndoes\\ndoesn't\\ndoing\\ndon't\\ndone\\ndown\\ndownwards\\nduring\\ne\\neach\\nedu\\neg\\neight\\neither\\nelse\\nelsewhere\\nenough\\nentirely\\nespecially\\net\\netc\\neven\\never\\nevery\\neverybody\\neveryone\\neverything\\neverywhere\\nex\\nexactly\\nexample\\nexcept\\nf\\nfar\\nfew\\nfifth\\nfirst\\nfive\\nfollowed\\nfollowing\\nfollows\\nfor\\nformer\\nformerly\\nforth\\nfour\\nfrom\\nfurther\\nfurthermore\\ng\\nget\\ngets\\ngetting\\ngiven\\ngives\\ngo\\ngoes\\ngoing\\ngone\\ngot\\ngotten\\ngreetings\\nh\\nhad\\nhadn't\\nhappens\\nhardly\\nhas\\nhasn't\\nhave\\nhaven't\\nhaving\\nhe\\nhe's\\nhello\\nhelp\\nhence\\nher\\nhere\\nhere's\\nhereafter\\nhereby\\nherein\\nhereupon\\nhers\\nherself\\nhi\\nhim\\nhimself\\nhis\\nhither\\nhopefully\\nhow\\nhowbeit\\nhowever\\ni\\ni'd\\ni'll\\ni'm\\ni've\\nie\\nif\\nignored\\nimmediate\\nin\\ninasmuch\\ninc\\nindeed\\nindicate\\nindicated\\nindicates\\ninner\\ninsofar\\ninstead\\ninto\\ninward\\nis\\nisn't\\nit\\nit'd\\nit'll\\nit's\\nits\\nitself\\nj\\njust\\nk\\nkeep\\nkeeps\\nkept\\nknow\\nknown\\nknows\\nl\\nlast\\nlately\\nlater\\nlatter\\nlatterly\\nleast\\nless\\nlest\\nlet\\nlet's\\nlike\\nliked\\nlikely\\nlittle\\nlook\\nlooking\\nlooks\\nltd\\nm\\nmainly\\nmany\\nmay\\nmaybe\\nme\\nmean\\nmeanwhile\\nmerely\\nmight\\nmore\\nmoreover\\nmost\\nmostly\\nmuch\\nmust\\nmy\\nmyself\\nn\\nname\\nnamely\\nnd\\nnear\\nnearly\\nnecessary\\nneed\\nneeds\\nneither\\nnever\\nnevertheless\\nnew\\nnext\\nnine\\nno\\nnobody\\nnon\\nnone\\nnoone\\nnor\\nnormally\\nnot\\nnothing\\nnovel\\nnow\\nnowhere\\no\\nobviously\\nof\\noff\\noften\\noh\\nok\\nokay\\nold\\non\\nonce\\none\\nones\\nonly\\nonto\\nor\\nother\\nothers\\notherwise\\nought\\nour\\nours\\nourselves\\nout\\noutside\\nover\\noverall\\nown\\np\\nparticular\\nparticularly\\nper\\nperhaps\\nplaced\\nplease\\nplus\\npossible\\npresumably\\nprobably\\nprovides\\nq\\nque\\nquite\\nqv\\nr\\nrather\\nrd\\nre\\nreally\\nreasonably\\nregarding\\nregardless\\nregards\\nrelatively\\nrespectively\\nright\\ns\\nsaid\\nsame\\nsaw\\nsay\\nsaying\\nsays\\nsecond\\nsecondly\\nsee\\nseeing\\nseem\\nseemed\\nseeming\\nseems\\nseen\\nself\\nselves\\nsensible\\nsent\\nserious\\nseriously\\nseven\\nseveral\\nshall\\nshe\\nshould\\nshouldn't\\nsince\\nsix\\nso\\nsome\\nsomebody\\nsomehow\\nsomeone\\nsomething\\nsometime\\nsometimes\\nsomewhat\\nsomewhere\\nsoon\\nsorry\\nspecified\\nspecify\\nspecifying\\nstill\\nsub\\nsuch\\nsup\\nsure\\nt\\nt's\\ntake\\ntaken\\ntell\\ntends\\nth\\nthan\\nthank\\nthanks\\nthanx\\nthat\\nthat's\\nthats\\nthe\\ntheir\\ntheirs\\nthem\\nthemselves\\nthen\\nthence\\nthere\\nthere's\\nthereafter\\nthereby\\ntherefore\\ntherein\\ntheres\\nthereupon\\nthese\\nthey\\nthey'd\\nthey'll\\nthey're\\nthey've\\nthink\\nthird\\nthis\\nthorough\\nthoroughly\\nthose\\nthough\\nthree\\nthrough\\nthroughout\\nthru\\nthus\\nto\\ntogether\\ntoo\\ntook\\ntoward\\ntowards\\ntried\\ntries\\ntruly\\ntry\\ntrying\\ntwice\\ntwo\\nu\\nun\\nunder\\nunfortunately\\nunless\\nunlikely\\nuntil\\nunto\\nup\\nupon\\nus\\nuse\\nused\\nuseful\\nuses\\nusing\\nusually\\nuucp\\nv\\nvalue\\nvarious\\nvery\\nvia\\nviz\\nvs\\nw\\nwant\\nwants\\nwas\\nwasn't\\nway\\nwe\\nwe'd\\nwe'll\\nwe're\\nwe've\\nwelcome\\nwell\\nwent\\nwere\\nweren't\\nwhat\\nwhat's\\nwhatever\\nwhen\\nwhence\\nwhenever\\nwhere\\nwhere's\\nwhereafter\\nwhereas\\nwhereby\\nwherein\\nwhereupon\\nwherever\\nwhether\\nwhich\\nwhile\\nwhither\\nwho\\nwho's\\nwhoever\\nwhole\\nwhom\\nwhose\\nwhy\\nwill\\nwilling\\nwish\\nwith\\nwithin\\nwithout\\nwon't\\nwonder\\nwould\\nwouldn't\\nx\\ny\\nyes\\nyet\\nyou\\nyou'd\\nyou'll\\nyou're\\nyou've\\nyour\\nyours\\nyourself\\nyourselves\\nz\\nzero\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words\n",
    "\n",
    "import requests\n",
    "words = [\"machine\", \"learning\", \"deep\", \"neural\", \"networks\", \"artificial\", \"intelligence\", \"data\", \"science\", \"python\", \"programming\", \"language\", \"model\", \"training\", \"algorithm\", \"accuracy\", \"precision\", \"recall\", \"evaluation\", \"metrics\"]\n",
    "\n",
    "txt = \" \".join(words)\n",
    "req = requests.get('https://raw.githubusercontent.com/mxr576/stopwords/refs/heads/master/en.txt')\n",
    "req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloudNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading wordcloud-1.9.4-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\slmss\\onedrive\\desktop\\a.i\\data-science-use-cases\\rnn\\.venv\\lib\\site-packages (from wordcloud) (2.0.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\slmss\\onedrive\\desktop\\a.i\\data-science-use-cases\\rnn\\.venv\\lib\\site-packages (from wordcloud) (11.0.0)\n",
      "Collecting matplotlib (from wordcloud)\n",
      "  Downloading matplotlib-3.9.3-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->wordcloud)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->wordcloud)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->wordcloud)\n",
      "  Downloading fonttools-4.55.3-cp311-cp311-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->wordcloud)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\slmss\\onedrive\\desktop\\a.i\\data-science-use-cases\\rnn\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (24.2)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->wordcloud)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\slmss\\onedrive\\desktop\\a.i\\data-science-use-cases\\rnn\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\slmss\\onedrive\\desktop\\a.i\\data-science-use-cases\\rnn\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.4-cp311-cp311-win_amd64.whl (299 kB)\n",
      "Downloading matplotlib-3.9.3-cp311-cp311-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.8 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/7.8 MB 2.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.8/7.8 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.8 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.8 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.4/7.8 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.2/7.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.0/7.8 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.5/7.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/7.8 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.3/7.8 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.8 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 2.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.3-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.7-cp311-cp311-win_amd64.whl (56 kB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, wordcloud\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.7 matplotlib-3.9.3 pyparsing-3.2.0 wordcloud-1.9.4\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n",
      "Trigrams: [('The', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    return n_grams\n",
    "\n",
    "# Example usage\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "bigrams = generate_ngrams(text, 2)\n",
    "trigrams = generate_ngrams(text, 3)\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(txt)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Convert the result to an array\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "# Get the feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"TF-IDF Feature names:\", tfidf_feature_names)\n",
    "print(\"TF-IDF array:\", tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.09193998174078082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "string1 = \"The zoo is tough terrain; hilly.\"\n",
    "string2 = \"We saw many annimals in zoo\"\n",
    "\n",
    "# Combine the strings into a list\n",
    "strings = [string1, string2]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the strings to TF-IDF features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(strings)\n",
    "\n",
    "# Calculate the cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "print(\"Cosine Similarity:\", cosine_sim[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install the sentence-transformers library\n",
    "# !pip install sentence-transformers\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the sentences\n",
    "embeddings = model.encode(strings)\n",
    "\n",
    "# Calculate the cosine similarity\n",
    "sbert_cosine_sim = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "\n",
    "print(\"SBERT Cosine Similarity:\", sbert_cosine_sim.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Neural networks are a powerful tool for machine learning\",\n",
    "    \"I enjoy learning about artificial intelligence\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create input sequences\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "# Create predictors and label\n",
    "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word_index = tf.argmax(predicted, axis=-1).numpy()[0]\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted_word_index:\n",
    "            return word\n",
    "    return \"\"\n",
    "\n",
    "# Predict a new corpus\n",
    "seed_text = \"I enjoy learning\"\n",
    "next_words = 5\n",
    "\n",
    "for _ in range(next_words):\n",
    "    next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len)\n",
    "    seed_text += \" \" + next_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
